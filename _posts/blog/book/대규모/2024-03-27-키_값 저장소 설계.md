---
title: "[책] 가상 면접 사례로 배우는 대규모 시스템 설계 기초 - 6장 키-값 저장소 설계"
last_modified_at: 2024-03-27
category: 프로그래밍 방법
tags:
  - 설계
---

키-값 데이터베이스라고도 불리는 비 관계형 데이터베이스. 성능상의 이유로 키는 짧을 수록 좋음
값 저장소 중 유명한 것으로는 아마존 다이나모, memcached, redis 등이 있음

서버가 한대 일 경우 구현은 간단함. 그러나 모든 데이터를 메모리안에 두는 것이 불가능할수도 있다는 약점을 가짐 -> 데이터 압축 or 자주쓰는 데이터만 메모리에 저장 or <mark class="hltr-cyan">분산 키-값 저장소 생성</mark>

#### 분산 키-값 저장소
분산 해시 테이블이라고도 불림
##### CAP정리
- 데이터 일관성(consistency) : 분산 시스템에 접속하는 모든 클라이언트는 어떤 노드에 접속했느냐에 관계없이 언제나 같은 데이터를 보게되어야함
- 가용성(availability) : 분산시스템에 접속하는 클라이언트는 일부 노드에 장애가 발생해도 항상 응답을 받을 수 있어야함
- 파티션 감내(partition tolerance) : 두 노드 사이에 통신 장애가 발생하였음을 의미. 네트워크에 파티션이 생기더라도 시스템은 계속 동작해야함
이들 가운데 <mark style="background: #ABF7F7A6;">어떤 두가지를 충족시키면 다른 하나는 반드시 희생되어야함</mark>
- CP 시스템 : 일관성과 파티션감내를 지원하나 가용성을 희생함
- AP 시스템 : 가용성과 파티션감내를 지원하나 데이터 일관성을 희생함
- CA 시스템 : 일관성과 가용성을 지원하나 파티션감내를 희생함. 그러나 통상 네트워크장애는 피할 수 없는 일로 여겨지기때문에 **분산 시스템은 반드시 파티션 문제를 감내할 수 있도록 설계되어야**한다. => *실세계에 CA 시스템은 존재하지않음*

이상적 환경에서 네트워크가 파티션되는 상황은 절대로 일어나지않음. n1에 기록된 데이터는 자동으로 n2, n3에 복제됨. 일관성, 가용성을 만족함
그러나 분산 시스템은 파티션 문제를 피할 수 없음. 파티션 문제 발생 시 일관성과 가용성 중 하나를 선택해야함
n3에 장애가 발생하여 n1, n2와 통신할 수 없는 상황에서 클라이언트가 n1, n2에 기록한 데이터는 n3에 전달되지않으며 n3에 기록되었으나 n1, n2에 전달되지않은 데이터는 알 수 없음
-> 가용성 대신 일관성 선택 시(CP)
	세 서버 사이 생길수 있는 데이터 불일치를 피하기 위해 n3의 장애 발생 시 n1, n2의 쓰기연산을 중단. 보통 은행권 시스템은 절대 일관성을 포기할 수 없어 선택함
-> 일관성 대신 가용성 선택(AP)
	파티션 문제가 해결 된 후에 노드간 데이터 동기화를 진행함
##### 시스템 컴포넌트
키-값 저장소 구현에 사용될 핵심 컴포넌트들 및 기술
- 데이터 파티션
	대규모 어플리케이션은 전체 데이터를 한대의 서버에 욱여넣는것이 불가능함. 데이터를 작은 파티션들로 분할해 여러 서버들에 저장하는것이 가장 단순한 해결책임. 그러나
	- 데이터를 여러 서버에 고르게 분산 가능?
	- 노드 추가/삭제될때 데이터이동 최소화 가능?
	위 2가지 문제를 해결하기 위해 안정 해시(consistent hash)를 사용 가능함
	안정해시를 사용해 데이터를 파티션하면 
	- 규모 확장 자동화 : 시스템부하에따라 서버가 자동 추가/삭제
	- 다양성 : 각 서버의 용량에 맞게 가상 노드의 수를 조정 가능. == 고성능 서버는 더 많은 가상 노드를 갖도록 설정가능함
- 데이터 다중화
	높은 가용성, 안정성 확보를 위해 데이터를 N개(튜닝 가능한 값)의 서버에 비동기적으로 다중화할 필요가있음
	어떤 키를 해시 링 위에 배치한 후 그 지점으로부터 시계방향으로 링을 순회하면서 만나는 첫 N개 서버에 데이터 사본을 복사
	가상 노드를 사용한다면 N개의 노드가 대응될 실제 물리서버의 서버의 개수보다 작아질 수도 있음
	=> **노드 선택 시 같은 물리 서버를 중복 선택하지않도록 해야함.**
- 데이터 일관성
	여러 노드의 데이터는 적절히 동기화되어야함
	정족수 합의 프로토콜을 사용하면 읽기/쓰기 연산 모두에 일관성을 보장 가능.
		- N = 사본갯수
		- W = 쓰기 연산에 대한 정족수. 쓰기연산이 성공한 것으로 간주되려면 적어도 W개의 서버로부터 쓰기연산이 성공했다는 응답을 받아야함
		- R = 읽기 연산에 대한 정족수. 읽기 연산이 성공한 것으로 간주되려면 적어도 R개의 서버로부터 응답을 받아야함
		![images](/assets/images/book/2024-03-23.11.44.43.png)
		**w=1** 은 쓰기연산이 성공했다고 판단하기 위해 <mark style="background: #ABF7F7A6;">중재자는 최소 한대의 서버로부터 쓰기성공응답을 받아야한다</mark>는 뜻 == s0으로부터 쓰기 성공 응답을 받았다면 s1, s2의 응답은 받을 필요없음
		여기서 중재자는 클라이언트와 노드 사이의 프록시 역할을 함
		w=1 or r=1은 중재자가 한대의 서버에서만 응답을 받으면 되니 응답속도는 빠르겠으나 일관성은 보장하기 힘들다는 단점이 있음
		- R=1, W=N : 빠른 읽기 연산 최적화
		- W=1, R=N : 빠른 쓰기 연산 최적화
		- W+R > N : 강한 일관성 보장(보통 N=3, W=R=2)
		- W+R <= N : 강한 일관성이 보장되지않음
		요구되는 일관성의 수준에 따라 적절히 조정하면됨
- 일관성 모델 : 데이터 일관성의 수준을 결정함.
	- 강한 일관성 : 모든 읽기 연산은 가장 최근에 갱신된 결과를 반환. == 클라이언트는 절대 낡은 데이터를 보지못함. 일반적으로 모든 사본에 현재쓰기연산의 결과가 반영될때까지 해당 데이터에대한 읽기/쓰기를 금지함(고 가용성 시스템에서는 부적합. 새요청 처리가 중단됨)
	- 약한 일관성 : 읽기연산은 가장 최근에 갱신된 결과를 반환하지 못할수도 있음
	- 최종 일관성 : 약한 일관성의 한 형태로 갱신 결과가 결국 모든 사본에 반영(동기화)되는 모델. 다이나모 또는 카산드라가 택하고있는 모델. 
		  쓰기연산이 병렬적으로 발생하며 시스템에 저장된 값의 일관성이 깨질수도 있음. 이 문제는 클라이언트가 해결해야함(데이터의 버전 정보를 활용해 일관성이 깨진 데이터를 읽지않도록 함)
- 비일관성 해소 기법 : 데이터 버저닝
	- 데이터를 생성할때마다 해당 데이터의 새로운 버전을 만드는것. 각 버전의 데이터는 변경이 불가능하다
	- 백터시계 : 버전 사이 충돌을 해소히가 위해 보편적으로 사용하는 기술. [서버, 버전] 의 순서 쌍을 데이터에 매단 것. 
		- D([S1, v1], [S2, v2], ..., [Sn, vn])와 같이 표현됨. 
		- D - 데이터, S - 서버번호, v - 버전 카운터
			- 데이터 D를 서버 s1에 기록
				  -> 있으면? vi 증가
				  -> 없으면? 새항목[Si, 1] 생성
			  1. 클라이언트가 D1시스템에 기록. S1이 쓰기 연산 처리
					-> D1([S1, 1]) 생성
			  2. 다른 클라이언트가 D1을 읽고 D2로 업데이트. S1이 쓰기 연산처리(기존 데이터 D1을 덮어씀)
					->D2([S1, 2]) 생성
			  3. 다른 클라이언트가 D2를 읽어 D3로 갱신, S2이 쓰기연산 
			     + 동시에 또 다른 클라이언트가 D2를 읽고 D4로 갱신, S3가 쓰기연산 처리
				  -> D3([S1, 2], [S2, 1]), D4([S1, 2], [S3, 1]) => 충돌 발생
			  4. 클라이언트가 D3과 D4를 읽으며 충돌 확인, 충돌된 데이터를 클라이언트가 해소 후 서버에 저장. 충돌 해소된 데이터를 S1이 기록
				  -> D5([S1, 3], [S2, 1], [S3, 1])
				=> 서버별 버전 중 어느 버전이 최신인지 확인하려면 버전의 합을 비교하면된다. 높은쪽이 최신버전. 만약 서버별 버전이 동일하다면 충돌한것.
				그러나 클라이언트에 버전 충돌 감지, 해소 로직이 들어가야해서 클라이언트 구현이 복잡해지며 서버:버전의 개수가 빨리늘어난다는것이 문제임. 해결하려면 길이에 임계치를 설정하고 길이가 길어지면 오래된 순서쌍을 백테시계에서 제거하는 로직을 추가해야하나 충돌해소과정의 효율성이 낮아짐. 
				그러나 아마존은 실제 서비스에서 해당문제가 발생한적이 없다고함. 일반적인 상황에서는 큰 문제요소는 아닌듯
 - 장애 처리
	 - 장애 감지 
		 - 멀티캐스팅 : 장애감지하기 가장 쉬운 방법이나 서버가 많을수록 비효율적
		 - 분산형 장애 감지 - 가십 프로토콜 
			 - 각 노드는 멤버십 목록을 유지(각 멤버 Id와 박동카운터 쌍의 목록)
			 - 각 노드는 주기적으로 자신의 박동 카운터를 증가
			 - 각 노드는 무작위로 선정된 노드들에게 주기적으로 자기 박동 카운터 목록을 보냄
			 - 박동 카운터 목록을 받은 노드는 멤버십 목록을 최신값으로 갱신
			 - 특정 멤버의 박동 카운터 값이 지정시간동안 갱신되지않으면 장애상태로 간주
	 - 일시적 장애 처리 : 가십프로토콜로 장애를 감지하면 가용성보장을 위해 조치를 취함
		 - 엄격한 정족수 : 읽기와 연산 금지
		 - 느슨한 정족수 : 조건을 완화하여 가용성을 높임. W개의 건강한 서버와 읽기연산을 수행할 건강한 서버 R개를 해시링에서 고름(장애서버 무시) -> 장애 서버로 가는 요청을 건강한 서버가 잠시 대신 처리함. 장애서버 복구 시 일괄 반영
		   일괄 반영을 위해 임시로 대신 요청을 수행한 서버는 단서(hint)를 남기는데 이런 방안을 **단서 후 임시 위탁(hinted handoff)기법** 이라 부름
   - 영구 장애 처리
	   - 반-엔트로피 프로토콜을 구현하여 사본을 동기화, 사본들을 비교하여 최신버전으로 갱신하는 과정 포함. 사본간의 일관성이 망가진 상태를 탐지, 전송 데이터의 양을 줄이기위해서는 **머클 트리** 사용
		   - 머클트리(Merkle, 해시트리): 각 노드에 그 자식 노드들에 보관된 값의 해시 or 자식노드들의 레이블로부터 계산된 해시값을 레이블로 붙여두는 트리
		     키 공간이 1~12일때 머클트리는
				![images](/assets/images/book/2024-03-23.16.55.40.png)
				- 1단계 : 키 공단을 버킷으로 나눔
				- 2단계 : 버킷이 포함된 각각의 키에 균등 분호 해시 함수를 적용 햐 해시값 계산
				- 3단계 : 버킷별로 해시값 계산, 해당 해시 값을 레이블로 갖는 노드 생성
				- 4단계 : 자식 노드의 레이블로부터 새로운 해시값을 계산하여 이진트리를 상향식 구성
				일관성이 깨진데이터 상자는 색깔로 표기함
				루트 노드의 해시값을 비교하는 것으로 머클트리의 비교를 시작. 루트노드의 해시값이 일치 == 두 서버는 같은 데이터를 갖음
				<mark style="background: #ABF7F7A6;">실제로 쓰이는 시스템의경우 버킷 하나의 크기가 꽤 크다</mark>
- 데이터 센터 장애 처리 : 정전, 네트워크 장애, 자연재해 등 다양한 이유로 발생가능. 데이터 센터 다중화 필요
- 시스템 아키텍처 다이어그램
	- 클라이언트 :  키-값 저장소가 제공하는 두 가지 단순한 api, 즉 get(key) 및 put(key, value)와 통신
	- 중재자 : 클라이언트에게 키-값 저장소에 프록시 역할을 하는 노드
	- 노드는 안정해시의 해시링 위에 분포
	- 노드를 자동추가, 삭제할수있으며 시스템은 완전히 분산됨
	- 데이터는 여러 노드에 다중화
	- 모든 노드가 같은 책임을 지므로 SPOF(Single Point Of Failure)는 존재하지않음
	- 완전히 분산된 설계이므로 모든 노드는 아래 기능을 전부 지원
		- 클라이언트 API
		- 장애 감지
		- 데이터 충돌 해소
		- 장애 복구 메커니즘
		- 다중화
		- 저장소 엔진
		- 그 외 기능
- 쓰기 경로 : 쓰기 요청이 특정 노드에 전달된다면?
	![images](/assets/images/book/2024-03-23.17.40.09.png)
	1. 쓰기 요청이 커밋로그 파일에 기록
	2. 데이터가 메모리 캐시에 기록
	3. 메모리캐시가 가득 차거나 사전에 정의한 임계치에 도달하면 데이터는 디스크에있는 SSTABLE(Sorted-String Table. <key, value>의 순서 쌍을 정렬된 리스트형태로 관리하는 테이블)에 기록됨
- 읽기 경로 : 요청을 받은 노드가 메모리에 데이터가있는지 확인 -> 있으면 클라로 바로 반환, 없으면 SSTable들 중 어디에 키가 있는지 찾아야함 -> 보통 블룸필터가 사용됨
	- 블룸필터(Bloom filter)
	![images](/assets/images/book/2024-03-23.17.48.41.png)
	1. 메모리캐시에 데이터 유무 확인
	2. 없으면 블룸필터 검사 -> 어떤 SSTable에 있는지 알아냄
	3. SSTable에서 데이터 가져옴
	4. 클라이언트에 반환